{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "from skimage import io\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d:\\\\nettech\\\\DeepLearning\\\\Datasets\\\\hand_signs\\\\data\\\\blank',\n",
       " 'd:\\\\nettech\\\\DeepLearning\\\\Datasets\\\\hand_signs\\\\data\\\\fist',\n",
       " 'd:\\\\nettech\\\\DeepLearning\\\\Datasets\\\\hand_signs\\\\data\\\\five',\n",
       " 'd:\\\\nettech\\\\DeepLearning\\\\Datasets\\\\hand_signs\\\\data\\\\ok',\n",
       " 'd:\\\\nettech\\\\DeepLearning\\\\Datasets\\\\hand_signs\\\\data\\\\thumbsdown',\n",
       " 'd:\\\\nettech\\\\DeepLearning\\\\Datasets\\\\hand_signs\\\\data\\\\thumbsup']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH =os.getcwd()+'\\\\Datasets\\\\hand_signs\\\\data'\n",
    "\n",
    "dataset_path = os.path.join(DATASET_PATH, '*')\n",
    "dataset_path = glob.glob(dataset_path)\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x250dbc999d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "image = cv2.imread(dataset_path[3] + '\\\\ok1.jpg')\n",
    "image = cv2.resize(image,(100, 120))\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\nettech\\DeepLearning//Datasets//hand_signs//data//blank d:\\nettech\\DeepLearning//Datasets//hand_signs//data//blank\\*\n",
      "d:\\nettech\\DeepLearning//Datasets//hand_signs//data//ok d:\\nettech\\DeepLearning//Datasets//hand_signs//data//ok\\*\n",
      "d:\\nettech\\DeepLearning//Datasets//hand_signs//data//thumbsup d:\\nettech\\DeepLearning//Datasets//hand_signs//data//thumbsup\\*\n",
      "d:\\nettech\\DeepLearning//Datasets//hand_signs//data//thumbsdown d:\\nettech\\DeepLearning//Datasets//hand_signs//data//thumbsdown\\*\n",
      "d:\\nettech\\DeepLearning//Datasets//hand_signs//data//fist d:\\nettech\\DeepLearning//Datasets//hand_signs//data//fist\\*\n",
      "d:\\nettech\\DeepLearning//Datasets//hand_signs//data//five d:\\nettech\\DeepLearning//Datasets//hand_signs//data//five\\*\n",
      "9600\n",
      "9600\n"
     ]
    }
   ],
   "source": [
    "loaded_images = []\n",
    "\n",
    "list_of_gestures = ['blank', 'ok', 'thumbsup', 'thumbsdown', 'fist', 'five']\n",
    "\n",
    "for path in range(0, 6):\n",
    "    dataset_path = os.getcwd()+'//Datasets//hand_signs//data//'+ str(list_of_gestures[path])\n",
    "    gesture_path = os.path.join(dataset_path, '*')\n",
    "    \n",
    "    print(dataset_path, gesture_path)\n",
    "    import glob\n",
    "    gest_path = glob.glob(gesture_path)\n",
    "    k = 0\n",
    "    for i in range(0, len(gest_path)):\n",
    "        if k < 1600:\n",
    "            image = cv2.imread(gest_path[i])\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            gray_image = cv2.resize(gray_image,(100, 120))\n",
    "            loaded_images.append(gray_image)\n",
    "        k=k+1\n",
    "print(len(loaded_images))\n",
    "\n",
    "outputVectors = []\n",
    "for i in range(1, 1601):\n",
    "    outputVectors.append([1, 0, 0, 0, 0, 0])\n",
    "\n",
    "for i in range(1, 1601):\n",
    "    outputVectors.append([0, 1, 0, 0, 0, 0])\n",
    "\n",
    "for i in range(1, 1601):\n",
    "    outputVectors.append([0, 0, 1, 0, 0, 0])\n",
    "    \n",
    "for i in range(1, 1601):\n",
    "    outputVectors.append([0, 0, 0, 1, 0, 0])\n",
    "    \n",
    "for i in range(1, 1601):\n",
    "    outputVectors.append([0, 0, 0, 0, 1, 0])\n",
    "\n",
    "for i in range(1, 1601):\n",
    "    outputVectors.append([0, 0, 0, 0, 0, 1])\n",
    "\n",
    "print(len(outputVectors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">118</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">118</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41216</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,275,776</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m118\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m118\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m57\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m57\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41216\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m5,275,776\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,295,750</span> (20.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,295,750\u001b[0m (20.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,295,558</span> (20.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,295,558\u001b[0m (20.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model\n",
    "model = Sequential()\n",
    "\n",
    "# first conv layer\n",
    "# input shape = (img_rows, img_cols, 1)\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(100,120, 1))) \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# second conv layer\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu')) # fully connected\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "optimiser = Adam() \n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9600, 120, 100)\n",
      "(9600, 6)\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(loaded_images)\n",
    "y = np.asarray(outputVectors)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7680, 100, 120, 1)\n",
      "(1920, 100, 120, 1)\n",
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - categorical_accuracy: 0.6383 - loss: 9.2022 - val_categorical_accuracy: 0.7891 - val_loss: 0.8814\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1s/step - categorical_accuracy: 0.8722 - loss: 0.3056 - val_categorical_accuracy: 0.9792 - val_loss: 0.1027\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1s/step - categorical_accuracy: 0.9008 - loss: 0.2355 - val_categorical_accuracy: 0.9651 - val_loss: 0.0763\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - categorical_accuracy: 0.9071 - loss: 0.2007 - val_categorical_accuracy: 0.9849 - val_loss: 0.0509\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - categorical_accuracy: 0.9350 - loss: 0.1473 - val_categorical_accuracy: 0.9891 - val_loss: 0.0331\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - categorical_accuracy: 0.9422 - loss: 0.1265 - val_categorical_accuracy: 0.9953 - val_loss: 0.0225\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - categorical_accuracy: 0.9480 - loss: 0.1211 - val_categorical_accuracy: 0.9922 - val_loss: 0.0313\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - categorical_accuracy: 0.9422 - loss: 0.1298 - val_categorical_accuracy: 0.9969 - val_loss: 0.0175\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - categorical_accuracy: 0.9514 - loss: 0.1272 - val_categorical_accuracy: 0.9870 - val_loss: 0.0546\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 1s/step - categorical_accuracy: 0.9567 - loss: 0.0994 - val_categorical_accuracy: 0.9927 - val_loss: 0.0244\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - categorical_accuracy: 0.9920 - loss: 0.0249\n",
      "Accuracy: 0.9927083253860474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "X_train = X_train.reshape(X_train.shape[0], 100, 120, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 100, 120, 1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "# Training the model with data\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "[loss, acc] = model.evaluate(X_test,y_test,verbose=1)\n",
    "print(\"Accuracy: \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"hand_gesture_recognition.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] please wait! calibrating...\n",
      "[STATUS] calibration successful...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# Global variables\n",
    "bg = None  # Background for running average calculation\n",
    "\n",
    "def run_avg(image, accumWeight):\n",
    "    global bg\n",
    "    if bg is None:\n",
    "        bg = image.copy().astype(\"float\")  # Initialize background\n",
    "        return\n",
    "    cv2.accumulateWeighted(image, bg, accumWeight)  # Update background with weighted average\n",
    "\n",
    "def segment(image, threshold=30):\n",
    "    global bg\n",
    "    diff = cv2.absdiff(bg.astype(\"uint8\"), image)  # Compute absolute difference between background and current frame\n",
    "    thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)[1]  # Apply threshold\n",
    "    cnts, _ = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)  # Find contours\n",
    "    if len(cnts) == 0:\n",
    "        return None  # Return None if no contours are found\n",
    "    else:\n",
    "        segmented = max(cnts, key=cv2.contourArea)  # Find the largest contour\n",
    "        return (thresholded, segmented)  # Return the thresholded image and the largest contour\n",
    "\n",
    "def _load_weights():\n",
    "    try:\n",
    "        model = load_model(\"hand_gesture_recognition.h5\")  # Load the pre-trained model\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(\"Error loading model:\", e)  # Print error if model loading fails\n",
    "        return None\n",
    "\n",
    "def getPredictedClass(model):\n",
    "    image = cv2.imread('Temp.png')  # Read the saved image\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    gray_image = cv2.resize(gray_image, (100, 120))  # Resize to the input size expected by the model\n",
    "    gray_image = gray_image.reshape(1, 100, 120, 1)  # Reshape for model input\n",
    "    prediction = model.predict_on_batch(gray_image)  # Predict the class\n",
    "    predicted_class = np.argmax(prediction)  # Get the index of the highest probability\n",
    "    \n",
    "    class_labels = [\"Blank\", \"OK\", \"Thumbs Up\", \"Thumbs Down\", \"Punch\", \"High Five\"]\n",
    "    return class_labels[predicted_class]  # Return the predicted class label\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    accumWeight = 0.8  # Weight for running average\n",
    "    camera = cv2.VideoCapture(1)  # Capture video from the first camera\n",
    "    fps = camera.get(cv2.CAP_PROP_FPS)  # Get the frames per second of the camera\n",
    "\n",
    "    top, right, bottom, left = 10, 350, 225, 590  # Region of interest (ROI) for hand detection\n",
    "    num_frames = 0  # Frame counter\n",
    "    model = _load_weights()  # Load the model\n",
    "    k = 0  # Frame counter for prediction\n",
    "    current_prediction = \"Blank\"  # Initial prediction\n",
    "    prediction_display_duration = 30  # Frames to display the prediction\n",
    "\n",
    "    if model is None:\n",
    "        print(\"Failed to load model. Exiting.\")\n",
    "        exit(1)  # Exit if model loading fails\n",
    "\n",
    "    while True:\n",
    "        grabbed, frame = camera.read()  # Capture a frame from the camera\n",
    "        if not grabbed:\n",
    "            break  # Exit the loop if frame not grabbed\n",
    "        frame = imutils.resize(frame, width=700)  # Resize the frame\n",
    "        frame = cv2.flip(frame, 1)  # Flip the frame horizontally\n",
    "        clone = frame.copy()  # Make a copy of the frame\n",
    "        roi = frame[top:bottom, right:left]  # Get the ROI\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)  # Convert ROI to grayscale\n",
    "        gray = cv2.GaussianBlur(gray, (7, 7), 0)  # Apply Gaussian blur to reduce noise\n",
    "\n",
    "        if num_frames < 30:\n",
    "            run_avg(gray, accumWeight)  # Update the background model with the running average\n",
    "            if num_frames == 1:\n",
    "                print(\"[STATUS] please wait! calibrating...\")\n",
    "            elif num_frames == 29:\n",
    "                print(\"[STATUS] calibration successful...\")\n",
    "        else:\n",
    "            hand = segment(gray)  # Segment the hand region\n",
    "            if hand is not None:\n",
    "                thresholded, segmented = hand\n",
    "                cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))  # Draw contours around the hand\n",
    "                if k % (fps // 6) == 0:  # Every 6th frame, save the thresholded image and predict\n",
    "                    cv2.imwrite('Temp.png', thresholded)\n",
    "                    current_prediction = getPredictedClass(model)\n",
    "                    k = 0\n",
    "                cv2.imshow(\"Thresholded\", thresholded)  # Show the thresholded image\n",
    "            else:\n",
    "                current_prediction = \"Blank\"  # No hand detected\n",
    "\n",
    "            if k < prediction_display_duration:  # Display the prediction for a set duration\n",
    "                cv2.putText(clone, str(current_prediction), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        k += 1  # Increment the frame counter for prediction\n",
    "        num_frames += 1  # Increment the total frame counter\n",
    "        cv2.rectangle(clone, (left, top), (right, bottom), (0, 255, 0), 2)  # Draw a rectangle around the ROI\n",
    "        cv2.imshow(\"Video Feed\", clone)  # Show the video feed\n",
    "        keypress = cv2.waitKey(1) & 0xFF  # Wait for a key press\n",
    "        if keypress == ord(\"q\"):  # Exit if 'q' is pressed\n",
    "            break\n",
    "\n",
    "    camera.release()  # Release the camera\n",
    "    cv2.destroyAllWindows()  # Close all OpenCV windows\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
